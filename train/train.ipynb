{"cells":[{"cell_type":"markdown","metadata":{"id":"iKZDuC4NdKZb"},"source":["## settings\n"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":2959,"status":"ok","timestamp":1728313172234,"user":{"displayName":"桂木悠作","userId":"03211982854032557089"},"user_tz":-540},"id":"bEBy1aiq0xP2"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')\n","\n","import os\n","import gc\n","import re\n","import time\n","import glob\n","import json\n","import pickle\n","import random\n","import shutil\n","import tarfile\n","import requests\n","from tqdm import tqdm\n","from collections import defaultdict\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.cuda.amp import autocast, GradScaler\n","from torch.utils.data import Dataset, DataLoader\n","\n","import transformers\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, BitsAndBytesConfig\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2248,"status":"ok","timestamp":1728313174474,"user":{"displayName":"桂木悠作","userId":"03211982854032557089"},"user_tz":-540},"id":"6kpIKuX3c6Mm","outputId":"f030c014-e986-4a59-b350-70b7ba2d4f80"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","rd = '/content/drive/MyDrive/Example/'\n","if not os.path.exists(rd):\n","  os.makedirs(rd)\n","\n","%cd $rd"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":25,"status":"ok","timestamp":1728313174475,"user":{"displayName":"桂木悠作","userId":"03211982854032557089"},"user_tz":-540},"id":"RbaJEwGcNIbK"},"outputs":[],"source":["!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1728313174475,"user":{"displayName":"桂木悠作","userId":"03211982854032557089"},"user_tz":-540},"id":"e8bEJ-7TpS4y","outputId":"6ad63b8a-7748-4130-8a2c-9fa6b0f3c48e"},"outputs":[{"name":"stdout","output_type":"stream","text":["File already exists\n"]}],"source":["# make 'data' folder (if not exists)\n","data_dir = './data/'\n","if not os.path.exists(data_dir):\n","  print(f'Making new folder: {data_dir}')\n","  os.mkdir(data_dir)\n","\n","\n","# load IMDb dataset\n","url = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n","file_name = 'aclImdb_v1.tar.gz'\n","src_path = '/content/extracted_files'\n","path_dict = {\n","    'train_pos': src_path + '/aclImdb/train/pos',\n","    'train_neg': src_path + '/aclImdb/train/neg',\n","    'test_pos': src_path + '/aclImdb/test/pos',\n","    'test_neg': src_path + '/aclImdb/test/neg'\n","}\n","dfs = []\n","\n","if not (os.path.isfile(data_dir + 'IMDb_train.tsv') and os.path.isfile(data_dir + 'IMDb_test.tsv')):\n","  print('Downloading...')\n","\n","  try:\n","    response = requests.get(url)\n","    response.raise_for_status()\n","    with open(file_name, 'wb') as f:\n","      f.write(response.content)\n","  except requests.exceptions.RequestException as e:\n","    print(f'Failed to download {file_name}: {e}')\n","\n","  print('Extracting...')\n","  with tarfile.open(file_name, 'r:gz') as tar:\n","    tar.extractall(path=src_path)\n","\n","  for k, v in path_dict.items():\n","    print(f'Concatenating: {k}')\n","    _dfs = []\n","\n","    for fname in tqdm(glob.glob(v + '/*.txt')):\n","      with open(fname, 'r', encoding='utf-8') as f:\n","        content = f.read()\n","      label = 1 if 'pos' in fname else 0\n","      df = pd.DataFrame({'label': [label], 'text': [content]})\n","      _dfs.append(df)\n","\n","    combined_df = pd.concat(_dfs, axis=0, ignore_index=True)\n","    dfs.append(combined_df)\n","\n","  print('Converting to tsv: train')\n","  df_train = pd.concat(dfs[:2], axis=0, ignore_index=True)\n","  df_train.to_csv(data_dir + 'IMDb_train.tsv', sep='\\t', index=False)\n","\n","  print('Converting to tsv: test')\n","  df_test = pd.concat(dfs[2:], axis=0, ignore_index=True)\n","  df_test.to_csv(data_dir + 'IMDb_test.tsv', sep='\\t', index=False)\n","\n","  print('Cleaning up...')\n","  shutil.rmtree(src_path)\n","  os.remove(file_name)\n","\n","  print('Done!')\n","\n","else:\n","  print('File already exists')\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1728313174475,"user":{"displayName":"桂木悠作","userId":"03211982854032557089"},"user_tz":-540},"id":"3ADBgSAc4zLn"},"outputs":[],"source":["class CFG:\n","  SEED = 42\n","  NUM_LABELS = 2\n","  MAX_LENGTH = 512\n","  EPOCHS = 3\n","  MAX_LR = 2e-5\n","  NUM_WARMUP_STEPS = 128\n","  DROPOUT = 0.05\n","  MODEL_NAME = 'microsoft/deberta-v3-base'\n","  DO_VAL = True\n","  DO_TRAIN = True\n","  DO_PREDICT = True\n","  DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","  DEVICE_NAME = ''\n","  BATCH_SIZE = 16\n","  if torch.cuda.is_available():\n","    DEVICE_NAME = torch.cuda.get_device_name(0)\n","  else:\n","    DEVICE_NAME = 'CPU'\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1728313174475,"user":{"displayName":"桂木悠作","userId":"03211982854032557089"},"user_tz":-540},"id":"-vDuXrym69ag"},"outputs":[],"source":["# seed\n","def set_seed(seed):\n","  random.seed(seed)\n","  np.random.seed(seed)\n","  os.environ['PYTHONHASHSEED'] = str(seed)\n","  torch.manual_seed(seed)\n","  torch.cuda.manual_seed(seed)\n","\n","  torch.backends.cudnn.deterministic = True\n","  torch.backends.cudnn.benchmark = True\n","\n","\n","set_seed(CFG.SEED)"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1728313174476,"user":{"displayName":"桂木悠作","userId":"03211982854032557089"},"user_tz":-540},"id":"g1H6ome986L5"},"outputs":[],"source":["# checkpoint\n","def save_checkpoint(model, optimizer, epoch, save_path):\n","  checkpoint = {\n","      'model': model.state_dict(),\n","      'optimizer': optimizer.state_dict(),\n","      'epoch': epoch\n","  }\n","  torch.save(checkpoint, save_path)\n","\n","\n","def load_checkpoint(model, optimizer, load_path):\n","  checkpoint = torch.load(load_path)\n","  model.load_state_dict(checkpoint['model'])\n","  optimizer.load_state_dict(checkpoint['optimizer'])\n","  epoch = checkpoint['epoch']\n","  return model, optimizer, epoch"]},{"cell_type":"markdown","metadata":{"id":"kiaGZQAt-_Yy"},"source":["## preprocessing"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1728313174476,"user":{"displayName":"桂木悠作","userId":"03211982854032557089"},"user_tz":-540},"id":"3XQ7Vyd0BxgS"},"outputs":[],"source":["pd.set_option('display.max_colwidth', None)"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":530,"status":"ok","timestamp":1728313174995,"user":{"displayName":"桂木悠作","userId":"03211982854032557089"},"user_tz":-540},"id":"rlTGhN0u92AA"},"outputs":[],"source":["train_path = rd + '/data/IMDb_train.tsv'\n","test_path =rd + '/data/IMDb_test.tsv'\n","\n","df_tr = pd.read_csv(train_path, sep='\\t')\n","df_ts = pd.read_csv(test_path, sep='\\t')\n"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":178},"executionInfo":{"elapsed":1728,"status":"ok","timestamp":1728313176718,"user":{"displayName":"桂木悠作","userId":"03211982854032557089"},"user_tz":-540},"id":"grLZhqN9CrGO","outputId":"9844d6b9-e296-439c-ab81-7dcc05e44d51"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>count</th>\n","    </tr>\n","    <tr>\n","      <th>label</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>257</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>255</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div><br><label><b>dtype:</b> int64</label>"],"text/plain":["label\n","0    257\n","1    255\n","Name: count, dtype: int64"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["def textPreprocessor(text):\n","  text = re.sub(r'[^.,a-zA-Z0-9\\s]', ' ', text)\n","  text = re.sub(r'<\\s*br\\s*/?\\s*>', '', text)\n","  text = text.replace('.', ' . ').replace(',', ' , ')\n","  return text\n","\n","\n","df_tr['text'] = df_tr['text'].apply(textPreprocessor)\n","df_ts['text'] = df_tr['text'].apply(textPreprocessor)\n","\n","# extract some data for predicting\n","num_shuffle = 20\n","shuffle_cnt = 0\n","\n","while shuffle_cnt < num_shuffle:\n","  df_ts = df_ts.sample(frac=1, random_state=CFG.SEED).reset_index(drop=True)\n","  shuffle_cnt += 1\n","\n","df_ts = df_ts.head(512)\n","df_ts.label.value_counts()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":126},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1728313176719,"user":{"displayName":"桂木悠作","userId":"03211982854032557089"},"user_tz":-540},"id":"q4opwszVP-vO","outputId":"2e20de45-2868-4d03-f05c-50f2c5904e9d"},"outputs":[],"source":["df_ts['text'].iloc[0]"]},{"cell_type":"markdown","metadata":{"id":"IkSKDhIDIq8S"},"source":["## tokenize"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1728313176719,"user":{"displayName":"桂木悠作","userId":"03211982854032557089"},"user_tz":-540},"id":"-2c1Tslkbw6L"},"outputs":[],"source":["out_dir = './outputs/'\n","check_dir = out_dir + 'checkpoint/'\n","\n","if not os.path.exists(out_dir):\n","  os.makedirs(out_dir)\n","\n","if not os.path.exists(check_dir):\n","  os.makedirs(check_dir)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2929,"status":"ok","timestamp":1728313179636,"user":{"displayName":"桂木悠作","userId":"03211982854032557089"},"user_tz":-540},"id":"OYkQ0anMH2ri","outputId":"601f0cea-1674-408c-da52-ceef44cdafaa"},"outputs":[{"data":{"text/plain":["('./outputs/tokenizer/tokenizer_config.json',\n"," './outputs/tokenizer/special_tokens_map.json',\n"," './outputs/tokenizer/spm.model',\n"," './outputs/tokenizer/added_tokens.json',\n"," './outputs/tokenizer/tokenizer.json')"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["# load tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(CFG.MODEL_NAME)\n","\n","# tokenizer settings\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = 'right'\n","tokenizer.add_special_tokens({'eos_token': tokenizer.eos_token})\n","\n","# save tokenizer\n","tokenizer.save_pretrained(out_dir + 'tokenizer')"]},{"cell_type":"markdown","metadata":{"id":"4yI69lUGLPtS"},"source":["## model settings"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1307,"status":"ok","timestamp":1728313180937,"user":{"displayName":"桂木悠作","userId":"03211982854032557089"},"user_tz":-540},"id":"MTkIbEwCLRoc","outputId":"c50a86ee-8dd2-4dde-e403-168e9b5b52b3"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["model = AutoModelForSequenceClassification.from_pretrained(CFG.MODEL_NAME,\n","                                                                num_labels=CFG.NUM_LABELS)\n","\n","model.config.pretraining_pt = 1\n","model.pad_token_id = tokenizer.pad_token_id"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1728313180937,"user":{"displayName":"桂木悠作","userId":"03211982854032557089"},"user_tz":-540},"id":"DKTZTfTQURj9"},"outputs":[],"source":["# we train all the parameters of model by default. Set layers to train depending on your training environment.\n","# it takes about 1 hour to fine-tune our base_model with L4-GPU.\n","\n","def set_trainable_layers(device_name):\n","  if 'Tesla T4' in device_name:\n","    CFG.BATCH_SIZE = 8\n","    for param in model.base_model.parameters():\n","      param.requires_grad = False\n","\n","    for param in model.classifier.parameters():\n","      param.requires_grad = True\n","\n","    for param in model.pooler.parameters():\n","      param.requires_grad = True\n","\n","    for param in model.deberta.encoder.layer[-1].parameters():\n","      param.requires_grad = True\n","\n","    for param in model.deberta.encoder.layer[-2].parameters():\n","      param.requires_grad = True\n","\n","    for param in model.deberta.encoder.layer[-3].parameters():\n","      param.requires_grad = True\n","    print(f'connecting to: {device_name}\\nbatch_size: {CFG.BATCH_SIZE}\\nsome layers were set to trainable.')\n","    print('If you encounter memory usage errors or other errors during training or prediction, please manually change the batch size or number of layers to train as needed.')\n","\n","  elif 'NVIDIA L4' in device_name:\n","    for param in model.base_model.parameters():\n","      param.requires_grad = True\n","    print(f'connecting to: {device_name}\\nbatch_size: {CFG.BATCH_SIZE}\\nall layers were set to trainable.')\n","\n","  elif 'NVIDIA A100' in device_name:\n","    CFG.BATCH_SIZE = 32\n","    for param in model.base_model.parameters():\n","      param.requires_grad = True\n","\n","  elif 'CPU' in device_name:\n","    _device = xm.xla_device()\n","    if 'xla' in str(_device):\n","      print('Warning: connecting to TPU runtime but our code is not optimized for xla device.')\n","      CFG.DEVICE = xm.xla_device()\n","      CFG.BATCH_SIZE = 16\n","      for param in model.base_model.parameters():\n","        param.requires_grad = False\n","\n","      for param in model.classifier.parameters():\n","        param.requires_grad = True\n","\n","      for param in model.pooler.parameters():\n","        param.requires_grad = True\n","\n","      for param in model.deberta.encoder.layer[-1].parameters():\n","        param.requires_grad = True\n","\n","      for param in model.deberta.encoder.layer[-2].parameters():\n","        param.requires_grad = True\n","\n","      for param in model.deberta.encoder.layer[-3].parameters():\n","        param.requires_grad = True\n","      print(f'connecting to: TPU\\nbatch_size: {CFG.BATCH_SIZE}\\nsome layers were set to trainable.')\n","      print('If you encounter memory usage errors or other errors during training or prediction, please manually change the batch size or number of layers to train as needed.')\n","\n","    else:\n","     raise RuntimeError(f'Unsupported device type: {device_name}. Please connect to a supported GPU Runtime or configure for your specific GPU.')\n","\n","  else:\n","    print('Warning: Using a CUDA-enabled GPU, but not one of the pre-configured types. Assuming compatibility and proceeding.')\n","    for param in model.base_model.parameters():\n","        param.requires_grad = True\n","    print(f'connecting to: {device_name}\\nbatch_size: {CFG.BATCH_SIZE}\\nall layers were set to trainable.')\n","    print('If you encounter memory usage errors or other errors during training or prediction, please manually change the batch size or number of layers to train as needed.')\n","\n","  total_params = sum(p.numel() for p in model.parameters())\n","  trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","  print('-' * 50)\n","  print(f\"Total parameters: {total_params}\")\n","  print(f\"Trainable parameters: {trainable_params}\")\n","  print(f\"Ratio: {(trainable_params / total_params) * 100}%\")\n","\n","\n","\n","set_trainable_layers(CFG.DEVICE_NAME)"]},{"cell_type":"markdown","metadata":{"id":"kMpRePsg2EqH"},"source":["## dataset, dataloader"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1728313180938,"user":{"displayName":"桂木悠作","userId":"03211982854032557089"},"user_tz":-540},"id":"bD1jNxH_2WZN"},"outputs":[],"source":["class CustomDataset(Dataset):\n","  def __init__(self, texts, labels, tokenizer, max_length):\n","    self.texts = texts\n","    self.labels = labels\n","    self.tokenizer = tokenizer\n","    self.max_length = max_length\n","\n","  def __len__(self):\n","    return len(self.labels)\n","\n","  def __getitem__(self, idx):\n","    if idx >= len(self.labels):\n","      raise IndexError(f\"Index {idx} is out of bounds for labels with length {len(self.labels)}\")\n","    text = self.texts[idx]\n","    label = self.labels[idx]\n","    token = self.tokenizer(text,\n","                            padding='max_length',\n","                            max_length=self.max_length,\n","                            truncation=True,\n","                            return_tensors='pt')\n","\n","    return token, text, label"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1728313180938,"user":{"displayName":"桂木悠作","userId":"03211982854032557089"},"user_tz":-540},"id":"zD_VH7O83Zyv","outputId":"25ba3ba8-a171-4391-be3b-23c80767961c"},"outputs":[],"source":["# DATA SPLIT\n","if CFG.DO_VAL:\n","  tr_txt,val_txt, tr_lbl, val_lbl = train_test_split(df_tr['text'].tolist(), df_tr['label'].tolist(), test_size=0.05, random_state=CFG.SEED)\n","  print(len(tr_txt))\n","  print(len(val_txt))\n","  print(len(tr_lbl))\n","  print(len(val_lbl))\n","\n","  # dataset\n","  tr_ds = CustomDataset(tr_txt, tr_lbl, tokenizer, CFG.MAX_LENGTH)\n","  val_ds = CustomDataset(val_txt, val_lbl, tokenizer, CFG.MAX_LENGTH)\n","  print(f\"Training dataset size: {len(tr_ds)}\")\n","  print(f\"Validation dataset size: {len(val_ds)}\")\n","\n","  # dataloader\n","  tr_dl = DataLoader(tr_ds, batch_size=CFG.BATCH_SIZE, shuffle=True)\n","  val_dl = DataLoader(val_ds, batch_size=CFG.BATCH_SIZE, shuffle=False)\n","  print(f\"Training dataloader size: {len(tr_dl)}\")\n","  print(f\"Validation dataloader size: {len(val_dl)}\")\n","\n","  dl_dict = {\n","      'train': tr_dl,\n","      'val': val_dl\n","  }\n","\n","  print(f'Training dataset size: {len(tr_ds)}')\n","  print(f'Validation dataset size: {len(val_ds)}')\n","\n","else:\n","  # dataset\n","  tr_ds = CustomDataset(df_tr['text'], df_tr['label'], tokenizer, CFG.MAX_LENGTH)\n","\n","  # dataloader\n","  tr_dl = DataLoader(tr_ds, batch_size=CFG.BATCH_SIZE, shuffle=True)\n","\n","  dl_dict = {\n","      'train': tr_dl\n","  }\n","\n","  print(f'Training dataset size: {len(tr_ds)}')"]},{"cell_type":"markdown","metadata":{"id":"Z9PIbAllQB0d"},"source":["## settings for training"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1728313180938,"user":{"displayName":"桂木悠作","userId":"03211982854032557089"},"user_tz":-540},"id":"cda6gREeQFgU","outputId":"09d0c3e2-ae37-40e9-c8e1-202d2a60f27f"},"outputs":[{"name":"stdout","output_type":"stream","text":["batch_size: 16\n","STEPS_PER_EPOCH: 1484\n","TOTAL_STEPS: 4452\n"]}],"source":["# optimzier and lr scheduler\n","N_SAMPLES = len(tr_ds)\n","STEPS_PER_EPOCH = N_SAMPLES // CFG.BATCH_SIZE\n","TOTAL_STEPS = CFG.EPOCHS * STEPS_PER_EPOCH\n","\n","optimizer = optim.AdamW(model.parameters(), lr=CFG.MAX_LR)\n","\n","lr_scheduler = transformers.get_cosine_schedule_with_warmup(\n","    optimizer=optimizer,\n","    num_warmup_steps=CFG.NUM_WARMUP_STEPS,\n","    num_training_steps=TOTAL_STEPS\n",")\n","\n","print(f'batch_size: {CFG.BATCH_SIZE}\\nSTEPS_PER_EPOCH: {STEPS_PER_EPOCH}\\nTOTAL_STEPS: {TOTAL_STEPS}')"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1728313180939,"user":{"displayName":"桂木悠作","userId":"03211982854032557089"},"user_tz":-540},"id":"12rFzrXx9gq_"},"outputs":[],"source":["# criterion\n","def criterion(logits, labels):\n","  return nn.BCEWithLogitsLoss()(logits, labels)"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1728313180940,"user":{"displayName":"桂木悠作","userId":"03211982854032557089"},"user_tz":-540},"id":"S7PjWL3P_Jqo"},"outputs":[],"source":["def trainer(model, dl_dict, criterion, optimizer, lr_scheduler, num_epochs):\n","  device = CFG.DEVICE\n","  compute_device = 'GPU' if device.type == 'cuda' else 'CPU'\n","  print(f'This is {compute_device} trainer!!')\n","  gc.collect()\n","  torch.cuda.empty_cache()\n","\n","  scaler = GradScaler()\n","  history = defaultdict(list)\n","\n","  start = time.time()\n","  model.to(device)\n","  for epoch in range(1, num_epochs+1):\n","    tr_dataset_size = 0\n","    tr_running_loss = 0.0\n","    tr_running_acc = 0.0\n","    val_dataset_size = 0\n","    val_running_loss = 0.0\n","    val_running_acc = 0.0\n","\n","    iteration = 0\n","\n","    # traine phase\n","    model.train()\n","    for batch in tqdm(dl_dict['train'], desc=f'Epoch:{epoch}/{num_epochs} | Phase:Train'):\n","      tokens, _, labels = batch\n","      input_ids = tokens['input_ids'].squeeze(1).to(CFG.DEVICE)\n","      attn_masks = tokens['attention_mask'].squeeze(1).to(CFG.DEVICE)\n","      labels = labels.to(CFG.DEVICE).unsqueeze(1)\n","      optimizer.zero_grad(set_to_none=True)\n","\n","      with autocast():\n","          outputs = model(input_ids=input_ids, attention_mask=attn_masks).logits\n","          outputs = outputs[:, 1].unsqueeze(1)\n","          loss = criterion(outputs, labels.float())\n","          _, preds = torch.max(outputs, 1)\n","\n","      scaler.scale(loss).backward()\n","      scaler.step(optimizer)\n","      scaler.update()\n","      lr_scheduler.step()\n","\n","      tr_acc = (torch.sum(preds == labels.squeeze().detach())).double() / input_ids.size(0)\n","      tr_running_loss += loss.item() * input_ids.size(0)\n","      tr_running_acc += tr_acc * input_ids.size(0)\n","      tr_dataset_size += input_ids.size(0)\n","      iteration += 1\n","\n","      if (iteration % 100) == 0:\n","          print(f'Phase:Train | Iteration:{iteration} | Loss:{loss.item()} | Accuracy:{tr_acc.item()}')\n","          print(f'Correct Predictions: {torch.sum(preds == labels.squeeze().detach()).double()}')\n","          print(f'Batch Size: {input_ids.size(0)}')\n","\n","      if (iteration % 500) == 0:\n","        print(f'\\nIteration:{iteration}')\n","        save_path = f'{check_dir}epoch_{epoch}_iter_{iteration}.pth'\n","        save_checkpoint(model, optimizer, epoch, save_path)\n","        print(f'\\nModel saved to: {save_path}')\n","\n","    if CFG.DO_VAL:\n","      model.eval()\n","      for batch in tqdm(dl_dict['val'], desc=f'Epoch:{epoch} | Phase:Validate'):\n","        tokens, _, labels = batch\n","        input_ids = tokens['input_ids'].squeeze(1).to(CFG.DEVICE)\n","        attn_masks = tokens['attention_mask'].squeeze(1).to(CFG.DEVICE)\n","        labels = labels.to(CFG.DEVICE).unsqueeze(1)\n","\n","        with torch.inference_mode():\n","          with autocast():\n","            outputs = model(input_ids=input_ids, attention_mask=attn_masks).logits\n","            outputs = outputs[:, 1].unsqueeze(1)\n","            loss = criterion(outputs, labels.float())\n","            _, preds = torch.max(outputs, 1)\n","\n","        val_acc = (torch.sum(preds == labels.squeeze().detach())).double() / input_ids.size(0)\n","        val_running_loss += loss.item() * input_ids.size(0)\n","        val_running_acc += val_acc * input_ids.size(0)\n","        val_dataset_size += input_ids.size(0)\n","\n","    # histoy\n","    tr_epoch_loss = tr_running_loss / tr_dataset_size\n","    tr_epoch_acc = tr_running_acc / tr_dataset_size\n","    history['train_loss'].append(tr_epoch_loss)\n","    history['train_auroc'].append(tr_epoch_acc)\n","    print(f'Epoch:{epoch}/{num_epochs} | Train Loss:{tr_epoch_loss} | Train ACC:{tr_epoch_acc}')\n","\n","    if CFG.DO_VAL:\n","      val_epoch_loss = val_running_loss / val_dataset_size\n","      val_epoch_acc = val_running_acc / val_dataset_size\n","      history['val_loss'].append(val_epoch_loss)\n","      history['val_auroc'].append(val_epoch_acc)\n","      print(f'Epoch:{epoch}/{num_epochs} | Val Loss:{val_epoch_loss} | Val ACC:{val_epoch_acc}')\n","\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","  end = time.time()\n","  time_elapsed = end - start\n","  print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n","      time_elapsed // 3600,\n","      (time_elapsed % 3600) // 60,\n","      (time_elapsed % 3600) % 60\n","  ))\n","\n","  return history, model\n"]},{"cell_type":"markdown","metadata":{"id":"w6dLwKvdf_-g"},"source":["## train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3822428,"status":"ok","timestamp":1728317004286,"user":{"displayName":"桂木悠作","userId":"03211982854032557089"},"user_tz":-540},"id":"d3FVWr9VfeZy","outputId":"0f9f8a15-7c6a-4fef-c063-ec220a9703e1"},"outputs":[],"source":["# train\n","if CFG.DO_TRAIN:\n","  history, trained_model = trainer(model, dl_dict, criterion, optimizer, lr_scheduler, CFG.EPOCHS)"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":2221,"status":"ok","timestamp":1728317006500,"user":{"displayName":"桂木悠作","userId":"03211982854032557089"},"user_tz":-540},"id":"S2wKIoyCePue"},"outputs":[],"source":["if CFG.DO_TRAIN:\n","  torch.save(trained_model.state_dict(), out_dir + 'trained_model.pth')"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1728317006501,"user":{"displayName":"桂木悠作","userId":"03211982854032557089"},"user_tz":-540},"id":"sWB2kBMrLmfw","outputId":"28afe833-39e2-4d40-aa2d-65f7cf26a6b8"},"outputs":[{"data":{"text/plain":["0"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["torch.cuda.empty_cache()\n","gc.collect()"]},{"cell_type":"markdown","metadata":{"id":"xt9c0TfIgOTA"},"source":["## predict"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2174,"status":"ok","timestamp":1728317008667,"user":{"displayName":"桂木悠作","userId":"03211982854032557089"},"user_tz":-540},"id":"0ZzO4QYJe0Xb","outputId":"48d94e4a-31be-4d3a-c834-ba49c373349a"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["if CFG.DO_PREDICT:\n","  batch_size = 4\n","  tokenizer = AutoTokenizer.from_pretrained(out_dir + 'tokenizer')\n","  model = AutoModelForSequenceClassification.from_pretrained(CFG.MODEL_NAME, num_labels=CFG.NUM_LABELS)\n","\n","  model.config.pretraining_pt = 1\n","  model.pad_token_id = tokenizer.pad_token_id\n","\n","  model.load_state_dict(torch.load(out_dir + 'trained_model.pth'))\n","  model.to(CFG.DEVICE)\n","  model.eval()\n","\n","  ts_ds = CustomDataset(df_ts['text'], df_ts['label'], tokenizer, CFG.MAX_LENGTH)\n","  ts_dl = DataLoader(ts_ds, batch_size=batch_size, shuffle=False, pin_memory=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1728317008668,"user":{"displayName":"桂木悠作","userId":"03211982854032557089"},"user_tz":-540},"id":"XLZf0Elu4Bqd","outputId":"b526cd6e-41de-42ba-fcc2-e23b2f308364"},"outputs":[],"source":["for batch in ts_dl:\n","  tokens, texts, labels = batch\n","  print(tokens)\n","  print(texts)\n","  break"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27667,"status":"ok","timestamp":1728317434665,"user":{"displayName":"桂木悠作","userId":"03211982854032557089"},"user_tz":-540},"id":"yRD-KJ_7AKjx","outputId":"cf3321e2-4b3e-43fe-f65f-a4607a18f891"},"outputs":[{"name":"stderr","output_type":"stream","text":["Phase:Predict: 100%|██████████| 128/128 [00:27<00:00,  4.70it/s]"]},{"name":"stdout","output_type":"stream","text":["Test data size: 512\n","Test accuracy: 99.21875\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["if CFG.DO_PREDICT:\n","  results = []\n","  model.to(CFG.DEVICE)\n","\n","  acc = 0.0\n","  dataset_size = 0\n","  for batch in tqdm(ts_dl, desc='Phase:Predict'):\n","    tokens, texts, labels = batch\n","    input_ids = tokens['input_ids'].squeeze(1).to(CFG.DEVICE)\n","    attn_masks = tokens['attention_mask'].squeeze(1).to(CFG.DEVICE)\n","    labels = labels.to(CFG.DEVICE)\n","\n","    with torch.inference_mode():\n","      with autocast():\n","        outputs = model(input_ids=input_ids, attention_mask=attn_masks, output_attentions=True)\n","        logits = outputs.logits\n","        attention = outputs.attentions[-1]\n","\n","      logits, preds = torch.max(logits, 1)\n","      acc += (torch.sum(preds == labels.detach())).double()\n","      dataset_size += input_ids.size(0)\n","\n","      batch_size, num_heads, seq_len, _ = attention.shape\n","      all_attn = torch.zeros((batch_size, CFG.MAX_LENGTH), device=attention.device)\n","\n","      for i in range(num_heads):\n","        all_attn += attention[:, i, 0, :]\n","      all_attn /= 12\n","      all_attn /= all_attn.max(dim=1, keepdim=True)[0]\n","\n","      probs = F.sigmoid(logits).cpu().numpy()\n","      preds = preds.cpu().numpy()\n","\n","      decoded_texts = [tokenizer.decode(ids, skip_special_tokens=False) for ids in input_ids]\n","\n","      for i in range(len(labels)):\n","        results.append({\n","            'pred': preds[i],\n","            'prob': probs[i],\n","            'label': labels[i].item(),\n","            'text': decoded_texts[i],\n","            'attention': all_attn[i].cpu().numpy()\n","        })\n","\n","    del input_ids, attn_masks, labels, outputs, logits, attention, probs, preds\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","\n","  print(f'Test data size: {dataset_size}')\n","  print(f'Test accuracy: {(acc / dataset_size) * 100}')\n"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":1033,"status":"ok","timestamp":1728317438606,"user":{"displayName":"桂木悠作","userId":"03211982854032557089"},"user_tz":-540},"id":"NQfWoVjFgmqE"},"outputs":[],"source":["for i in range(len(results)):\n","  text = results[i]['text']\n","  text = re.sub(r'\\[SEP\\]', '', text)\n","  text = re.findall(r'\\[CLS\\]|\\w+|[.,]', text)\n","  text.append('[SEP]')\n","  results[i]['text'] = text\n","\n","for i in range(len(results)):\n","  attns = results[i]['attention']\n","  attns = attns[attns != 0.0]\n","  results[i]['attention'] = attns"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1694,"status":"ok","timestamp":1728317508918,"user":{"displayName":"桂木悠作","userId":"03211982854032557089"},"user_tz":-540},"id":"mSDEAHFLdVgo","outputId":"559cba4b-b48a-44ef-b2a5-f20cdb1a8608"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 512/512 [00:00<00:00, 77269.85it/s]\n"]}],"source":["def make_serializable(results):\n","    serializable_results = []\n","    for item in tqdm(results):\n","        serializable_item = {\n","            'pred': int(item['pred']),\n","            'prob': float(item['prob']),\n","            'label': int(item['label']),\n","            'text': item['text'],\n","            'attention': item['attention'].tolist()\n","        }\n","        serializable_results.append(serializable_item)\n","    return serializable_results\n","\n","if CFG.DO_PREDICT:\n","  results = make_serializable(results)\n","\n","  with open(out_dir + 'results.json', 'w') as f:\n","    json.dump(results, f, ensure_ascii=False, indent=4)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyP791c62SBMC36mXWn/pXvq","gpuType":"L4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
